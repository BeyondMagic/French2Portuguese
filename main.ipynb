{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Trabalho 1 - T√≥picos Especiais em Matem√°tica Aplicada\n",
        "\n",
        "**Alunos/Matricula:** Jo√£o V. Farias & Renan V. Guedes / 221022604 & 221031363\n",
        "\n",
        "**Arquitetura Usada:** Encoder-Decoder\n",
        "\n",
        "**Dataset Link:** V1: [D-Talk](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatefr_to_pt) from TensorFlow.Datasets\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0iu8uQIc-P33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Projeto para traduzir mensagens do Franc√™s para o Portugues**  \n",
        "\n",
        "Neste projeto, vamos explorar e comparar tr√™s arquiteturas de redes neurais para tradu√ß√£o autom√°tica do franc√™s para o portugu√™s, usando o *dataset* TED Talks do *Open Translation Project*. A ideia √© testar modelos do tipo **Encoder-Decoder**, analisando suas diferen√ßas e impacto na qualidade da tradu√ß√£o.  \n",
        "\n",
        "Os tr√™s modelos que vamos treinar s√£o:  \n",
        "\n",
        "1. **LSTM (Long Short-Term Memory)**  \n",
        "   - Um modelo b√°sico de rede recorrente bidirecional. O **Encoder** processa a frase em franc√™s e gera um contexto, enquanto o **Decoder** usa esse contexto para formar a tradu√ß√£o em portugu√™s.  \n",
        "   - A principal vantagem desse modelo √© sua capacidade de lidar com depend√™ncias de longo prazo nas sequ√™ncias.  \n",
        "\n",
        "2. **LSTM com Mecanismos de Aten√ß√£o**  \n",
        "   - Uma vers√£o aprimorada do modelo anterior, adicionando camadas de aten√ß√£o (produto escalar, Bahdanau e Luong).  \n",
        "   - A aten√ß√£o ajuda o modelo a \"olhar\" para partes espec√≠ficas da frase de entrada enquanto traduz, melhorando a coer√™ncia e precis√£o.  \n",
        "\n",
        "3. **Transformers**  \n",
        "   - Uma abordagem mais moderna, baseada em **autoaten√ß√£o**, eliminando o uso de redes recorrentes.  \n",
        "   - Trabalha com processamento paralelo, usando *Multi-Head Attention* e *Positional Encoding* para entender rela√ß√µes entre palavras, mesmo quando est√£o distantes na frase.  \n",
        "\n",
        "**Como vamos testar os modelos?**  \n",
        "- **Dataset**: Vamos usar cerca de 52.000 pares de frases (franc√™s-portugu√™s) para treinar, al√©m de 1.200 para valida√ß√£o e 1.800 para teste.  \n",
        "- **Pr√©-processamento**: Faremos a tokeniza√ß√£o com *SubwordTextEncoder* para reduzir palavras fora do vocabul√°rio (*out-of-vocabulary* ‚Äì OOV).  \n",
        "- **Treinamento**: Otimiza√ß√£o com Adam, acompanhando a perda (*loss*) e a acur√°cia durante o processo.  \n",
        "- **Avalia√ß√£o**: Vamos comparar os resultados usando a m√©trica BLEU e analisar exemplos pr√°ticos das tradu√ß√µes.  \n",
        "\n",
        "**O que esperamos encontrar?**  \n",
        "- Nosso objetivo √© entender qual desses modelos tem o melhor equil√≠brio entre qualidade de tradu√ß√£o e efici√™ncia computacional.  \n",
        "- √â prov√°vel que os Transformers tenham um desempenho superior, j√° que conseguem processar frases de forma mais eficiente, enquanto os modelos com LSTM e aten√ß√£o devem mostrar um avan√ßo significativo sobre a vers√£o b√°sica de LSTM.  \n",
        "\n",
        "No fim das contas, essa an√°lise pode ajudar a compreender melhor como diferentes abordagens de deep learning se saem em tarefas de tradu√ß√£o, trazendo insights √∫teis para aplica√ß√µes reais em NLP.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qqx0oHhH_YkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Importando as bibliotecas necess√°rias"
      ],
      "metadata": {
        "id": "rO-pcdsZGM-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o1RAVeffMKsD",
        "outputId": "70dee151-a714-40ad-80ad-a0cb9d1b9543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Primeiro, vamos importar todas as bibliotecas que vamos precisar ao longo do Projeto\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Para processamento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Componentes do Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, MultiHeadAttention, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Para visualiza√ß√£o dos resultados\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tf.config.optimizer.set_jit(True)  # Ativa o XLA JIT compilation\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16') # For√ßa o TensorFlow a usar precis√£o mista\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Carregando e Preparando os Dados"
      ],
      "metadata": {
        "id": "epn56848Le9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/fr_to_pt', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGQjWQARLi89",
        "outputId": "34f22e00-a45f-4d3c-cc9e-9af67d296825"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataset carregado e preprocessado com sucesso! üéâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tf.strings.regex_replace(text, r\"([?.!,¬ø])\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r'[\" \"]+', \" \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "oDN1N2pQnAct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(examples, max_samples=None):\n",
        "    fr_texts, pt_texts = [], []\n",
        "    for fr, pt in examples:\n",
        "        fr_texts.append(preprocess_text(fr).numpy().decode('utf-8'))\n",
        "        pt_texts.append(preprocess_text(pt).numpy().decode('utf-8'))\n",
        "        if max_samples and len(fr_texts) >= max_samples:\n",
        "            break\n",
        "    return fr_texts, pt_texts"
      ],
      "metadata": {
        "id": "grtXEGGOm-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fr_texts, pt_texts = prepare_dataset(train_examples, max_samples=50000)"
      ],
      "metadata": {
        "id": "qtcSYCjmneb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_fr_texts, val_pt_texts = prepare_dataset(val_examples, max_samples=10000)"
      ],
      "metadata": {
        "id": "40mlJIMsnrGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Configurando os Tokenizers"
      ],
      "metadata": {
        "id": "BlMfvl6wMzjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(fr_texts, pt_texts, max_input_length, max_target_length):\n",
        "    tokenizer_fr = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_fr.fit_on_texts(fr_texts)\n",
        "    input_vocab_size = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "    tokenizer_pt = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_pt.fit_on_texts(pt_texts)\n",
        "    target_vocab_size = len(tokenizer_pt.word_index) + 1\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    fr_sequences = tokenizer_fr.texts_to_sequences(fr_texts)\n",
        "    pt_sequences = tokenizer_pt.texts_to_sequences(pt_texts)\n",
        "\n",
        "    # Prepare decoder inputs and outputs\n",
        "    decoder_inputs = [seq[:-1] for seq in pt_sequences]\n",
        "    decoder_outputs = [seq[1:] for seq in pt_sequences]\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_inputs = pad_sequences(fr_sequences, maxlen=max_input_length, padding='post')\n",
        "    decoder_inputs = pad_sequences(decoder_inputs, maxlen=(max_target_length - 1), padding='post')\n",
        "    decoder_outputs = pad_sequences(decoder_outputs, maxlen=(max_target_length - 1), padding='post')\n",
        "\n",
        "    return encoder_inputs, decoder_inputs, decoder_outputs, input_vocab_size, target_vocab_size\n"
      ],
      "metadata": {
        "id": "2dHvGPlSfaaX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 50\n",
        "max_target_length = 50"
      ],
      "metadata": {
        "id": "yBXIIu7XqYEh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_inputs, train_decoder_inputs, train_decoder_outputs, input_vocab_size, target_vocab_size = tokenize_and_pad(\n",
        "    fr_texts, pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "tqLUNrnaqOgZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encoder_inputs, val_decoder_inputs, val_decoder_outputs, _, _ = tokenize_and_pad(\n",
        "    val_fr_texts, val_pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "esAVoW4ln0oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer_pt.word_index.get('[START]')"
      ],
      "metadata": {
        "id": "aGxzDq-R94xz"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_fr_sequences = tokenizer_fr.texts_to_sequences(fr_texts)\n",
        "#train_pt_sequences = tokenizer_pt.texts_to_sequences(pt_texts)"
      ],
      "metadata": {
        "id": "VIGkUTcdqMd6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleciona algumas amostras do dataset de treino pr√©-processado\n",
        "#for i, (fr, pt) in enumerate(train_dataset.take(3)):\n",
        "#    print(f\"\\nAmostra {i+1}:\")\n",
        "#    print(\"Franc√™s:  \", fr.numpy().decode('utf-8'))\n",
        "#    print(\"Portugu√™s:\", pt.numpy().decode('utf-8'))\n",
        "#\n",
        "#val_fr_texts, val_pt_texts = [], []\n",
        "#for fr, pt in val_dataset.take(MAX_SAMPLES):\n",
        "#    val_fr_texts.append(fr.numpy().decode('utf-8'))\n",
        "#    val_pt_texts.append(pt.numpy().decode('utf-8'))\n",
        "#\n",
        "#val_fr_sequences = tokenizer_fr.texts_to_sequences(val_fr_texts)\n",
        "#val_pt_sequences = tokenizer_pt.texts_to_sequences(val_pt_texts)\n"
      ],
      "metadata": {
        "id": "a8ZRULeI1Hq0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#max_input_length = max(len(seq) for seq in train_fr_sequences)\n",
        "#max_target_length = max(len(seq) for seq in train_pt_sequences)"
      ],
      "metadata": {
        "id": "z7nvo5BnM0Wg"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_decoder_inputs = [seq[:-1] for seq in train_pt_sequences]\n",
        "#train_decoder_outputs = [seq[1:] for seq in train_pt_sequences]\n",
        "#\n",
        "#val_decoder_inputs = [seq[:-1] for seq in val_pt_sequences]\n",
        "#val_decoder_outputs = [seq[1:] for seq in val_pt_sequences]"
      ],
      "metadata": {
        "id": "PoM1bXiMqBdO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequ√™ncias pad\n",
        "#train_encoder_inputs = pad_sequences(train_fr_sequences, maxlen=max_input_length, padding='post')\n",
        "#train_decoder_inputs = pad_sequences(train_decoder_inputs, maxlen=(max_target_length-1), padding='post')\n",
        "#train_decoder_outputs = pad_sequences(train_decoder_outputs, maxlen=(max_target_length-1), padding='post')\n",
        "#\n",
        "#val_encoder_inputs = pad_sequences(val_fr_sequences, maxlen=max_input_length, padding='post')\n",
        "#val_decoder_inputs = pad_sequences(val_decoder_inputs, maxlen=(max_target_length-1), padding='post')\n",
        "#val_decoder_outputs = pad_sequences(val_decoder_outputs, maxlen=(max_target_length-1), padding='post')"
      ],
      "metadata": {
        "id": "KH9Sipk16w4O"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Modelo 1: LSTM B√°sico\n",
        "### Vamos come√ßar com o modelo mais simples: um **Encoder-Decoder** usando **LSTM**"
      ],
      "metadata": {
        "id": "Efz62L4hU--R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_vocab_size, target_vocab_size,\n",
        "                     max_input_len, max_target_len,\n",
        "                     latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,), name=\"encoder_inputs\")\n",
        "    encoder_embed = Embedding(input_vocab_size, embedding_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_state=True, name=\"encoder_lstm\")\n",
        "    _, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "\n",
        "    # Decoder (teacher forcing during training)\n",
        "    decoder_inputs = Input(shape=(max_target_len-1,), name=\"decoder_inputs\")\n",
        "    decoder_embed = Embedding(target_vocab_size, embedding_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True, name=\"decoder_lstm\")\n",
        "    decoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32', name=\"decoder_dense\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Build the full model and attach useful layers for inference.\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.encoder_inputs = encoder_inputs\n",
        "    model.encoder_embedding = model.get_layer(\"encoder_embedding\")\n",
        "    model.encoder_lstm = model.get_layer(\"encoder_lstm\")\n",
        "    model.decoder_embedding = model.get_layer(\"decoder_embedding\")\n",
        "    model.decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
        "    model.decoder_dense = decoder_dense\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "M6uiZBc2WLQF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando e Compilando o Modelo"
      ],
      "metadata": {
        "id": "O-GvW_reWVTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = build_lstm_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_input_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6eN7iyFZWXkf",
        "outputId": "2d06b030-da6e-403c-ad0e-ddfcc6130541"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ encoder_inputs            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m)            ‚îÇ              \u001b[38;5;34m0\u001b[0m ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_inputs            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            ‚îÇ              \u001b[38;5;34m0\u001b[0m ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_embedding         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ      \u001b[38;5;34m9,464,576\u001b[0m ‚îÇ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mEmbedding\u001b[0m)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_embedding         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ      \u001b[38;5;34m9,603,328\u001b[0m ‚îÇ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mEmbedding\u001b[0m)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   ‚îÇ        \u001b[38;5;34m525,312\u001b[0m ‚îÇ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
              "‚îÇ                           ‚îÇ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ        \u001b[38;5;34m525,312\u001b[0m ‚îÇ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ cast_2 (\u001b[38;5;33mCast\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ              \u001b[38;5;34m0\u001b[0m ‚îÇ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m37513\u001b[0m)     ‚îÇ      \u001b[38;5;34m9,640,841\u001b[0m ‚îÇ cast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)              </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">        Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to           </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ encoder_inputs            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>)            ‚îÇ              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_inputs            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            ‚îÇ              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_embedding         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,464,576</span> ‚îÇ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_embedding         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,603,328</span> ‚îÇ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> ‚îÇ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
              "‚îÇ                           ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> ‚îÇ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37513</span>)     ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,640,841</span> ‚îÇ cast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Modelo"
      ],
      "metadata": {
        "id": "_4MEPbe8Wb9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "history = lstm_model.fit(\n",
        "    [train_enc, train_dec_in], train_dec_out,\n",
        "    validation_data=prepare_datasets(fr_tokenizer, pt_tokenizer, val_dataset, MAX_SAMPLES)[0],\n",
        "    #[train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\n",
        "    #validation_data=([val_encoder_inputs, val_decoder_inputs], val_decoder_outputs),\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "vfhCfnnHWe7v",
        "outputId": "24485f9b-059f-4e28-954f-edaf0fd7cd0d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4388/4388\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8836 - loss: 1.8984"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-674e7d1cc367>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = lstm_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#[train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Modelo 2: LSTM (Luong)\n"
      ],
      "metadata": {
        "id": "yNXz1Ebijgla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W(query + values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "P92Vd5FsjfaL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_luong_attention_model(input_vocab_size, target_vocab_size, max_source_len, max_target_len,\n",
        "                                    latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_source_len,))\n",
        "    enc_emb = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_sequences=True, return_state=True)\n",
        "    enc_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "    # Decoder with Attention\n",
        "    decoder_inputs = Input(shape=(max_target_len-1,))\n",
        "    dec_emb = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True, return_state=True)\n",
        "    dec_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    attention = LuongAttention(latent_units)\n",
        "    context_vector, _ = attention(dec_outputs, enc_outputs)\n",
        "    dec_outputs = tf.concat([context_vector, dec_outputs], axis=-1)\n",
        "\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32')\n",
        "    outputs = decoder_dense(dec_outputs)\n",
        "\n",
        "    return Model([encoder_inputs, decoder_inputs], outputs)"
      ],
      "metadata": {
        "id": "52gXid9Uk7kq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model = build_lstm_luong_attention_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_source_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "# optimizer=Adam(learning_rate=0.001)\n",
        "lstm_attn_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_attn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "dePkqj9kl2OZ",
        "outputId": "a5039c06-c3f7-4d71-ca5f-7eed5d0380f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'luong_attention' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Dimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'luong_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  ‚Ä¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  ‚Ä¢ kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8865d15aeb9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lstm_attn_model = build_lstm_luong_attention_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_source_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_target_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-c8788a8483d6>\u001b[0m in \u001b[0;36mbuild_lstm_luong_attention_model\u001b[0;34m(input_vocab_size, target_vocab_size, max_source_len, max_target_len, latent_units, embedding_dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLuongAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdec_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-ceb4c796fc11>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, query, values)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  ‚Ä¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  ‚Ä¢ kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model.fit(\n",
        "    [train_enc, train_dec_in], train_dec_out,\n",
        "    validation_data=prepare_datasets(fr_tokenizer, pt_tokenizer, val_dataset, MAX_SAMPLES)[0],\n",
        "    # validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "FHfBYEr1aGzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Visualizando os Resultados"
      ],
      "metadata": {
        "id": "im8JfjrfWjMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot da loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Valida√ß√£o')\n",
        "    plt.title('Loss ao Longo do Treinamento')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot da acur√°cia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Valida√ß√£o')\n",
        "    plt.title('Acur√°cia ao Longo do Treinamento')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Acur√°cia')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "70HJqHy1Wl86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_by_position(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plots the token-level accuracy at each sequence position.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the ground truth token indices.\n",
        "      - y_pred: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the predicted token indices.\n",
        "    \"\"\"\n",
        "    seq_length = y_true.shape[1]\n",
        "    accuracies = []\n",
        "    for pos in range(seq_length):\n",
        "        # Compute the fraction of tokens predicted correctly at this position.\n",
        "        pos_accuracy = np.mean(y_true[:, pos] == y_pred[:, pos])\n",
        "        accuracies.append(pos_accuracy)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(seq_length), accuracies, marker='o')\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy by Token Position\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Y-E3-rAeZ41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_length_distribution(y_true, y_pred):\n",
        "    # Calculando comprimentos (ignorando padding)\n",
        "    true_lengths = [len([x for x in seq if x != 0]) for seq in y_true]\n",
        "    pred_lengths = [len([x for x in seq if x != 0]) for seq in y_pred]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist([true_lengths, pred_lengths], label=['Real', 'Previsto'],\n",
        "             alpha=0.7, bins=20)\n",
        "    plt.title('Distribui√ß√£o do Comprimento das Tradu√ß√µes')\n",
        "    plt.xlabel('Comprimento da Sequ√™ncia')\n",
        "    plt.ylabel('Frequ√™ncia')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8tlGUuaWb1i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_heatmap(y_true, y_pred, token_labels):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix (as a heatmap) for token predictions.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of token indices (can be 2D or flattened)\n",
        "      - y_pred: numpy array of token indices (can be 2D or flattened)\n",
        "      - token_labels: list of strings that maps each token index to a label.\n",
        "                      For example: [\"PAD\", \"[START]\", \"[END]\", \"bonjour\", ...]\n",
        "    \"\"\"\n",
        "    # If the inputs are 2D (num_samples x seq_length), flatten them.\n",
        "    if y_true.ndim > 1:\n",
        "        y_true = y_true.flatten()\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # Compute the confusion matrix.\n",
        "    labels = np.arange(len(token_labels))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=token_labels,\n",
        "                yticklabels=token_labels)\n",
        "    plt.xlabel(\"Predicted Token\")\n",
        "    plt.ylabel(\"True Token\")\n",
        "    plt.title(\"Confusion Matrix of Token Predictions\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q7I9QRrpb32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, assume we have 100 samples, each of length 10 tokens.\n",
        "num_samples = 100\n",
        "sequence_length = 10\n",
        "vocab_size = 50  # For example, assume your vocabulary has 50 tokens (indices 0 to 49)\n",
        "\n",
        "# Create some random demo ground truth and predicted token sequences.\n",
        "np.random.seed(42)\n",
        "\n",
        "# To-do: real data here.\n",
        "y_true_demo = np.random.randint(0, vocab_size, size=(num_samples, sequence_length))\n",
        "y_pred_demo = np.random.randint(0, vocab_size, size=(num_samples, sequence_length))\n",
        "\n",
        "# Create a list of token labels for the confusion heatmap.\n",
        "# (In your case, you might use tokenizer_pt.index_word but here we simulate labels.)\n",
        "token_labels = [f\"Token {i}\" for i in range(vocab_size)]\n",
        "\n",
        "# Plot accuracy by token position.\n",
        "plot_accuracy_by_position(y_true_demo, y_pred_demo)\n",
        "\n",
        "# Plot confusion heatmap.\n",
        "plot_confusion_heatmap(y_true_demo, y_pred_demo, token_labels)"
      ],
      "metadata": {
        "id": "Vg1MAdWxZ-UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Acur√°cia por posi√ß√£o\n",
        "val_data = [train_encoder_inputs[-1000:], train_decoder_inputs[-1000:]]  # Usando √∫ltimas 1000 amostras como valida√ß√£o\n",
        "plot_accuracy_by_position(model, val_data[0], val_data[1])\n",
        "\n",
        "# 2. Distribui√ß√£o de comprimentos\n",
        "predictions = model.predict([val_data[0], val_data[1][:, :-1]])\n",
        "pred_classes = np.argmax(predictions, axis=-1)\n",
        "plot_length_distribution(val_data[1][:, 1:], pred_classes)\n",
        "\n",
        "# 3. Heatmap de confus√£o\n",
        "plot_confusion_heatmap(val_data[1][:, 1:], pred_classes, tokenizer_pt)\n",
        "\n",
        "print(\"\\nLegenda das visualiza√ß√µes:\")\n",
        "print(\"1. Gr√°fico de Acur√°cia por Posi√ß√£o: Mostra como o modelo se comporta em diferentes posi√ß√µes da sequ√™ncia\")\n",
        "print(\"2. Distribui√ß√£o de Comprimentos: Compara o tamanho das tradu√ß√µes reais vs. previstas\")\n",
        "print(\"3. Heatmap: Mostra quais palavras frequentes s√£o mais confundidas entre si\")"
      ],
      "metadata": {
        "id": "seE3fGeabeMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}