{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Trabalho 1 - TÃ³picos Especiais em MatemÃ¡tica Aplicada\n",
        "\n",
        "**Alunos/Matricula:** JoÃ£o V. Farias & Renan V. Guedes / 221022604 & 221031363\n",
        "\n",
        "**Arquitetura Usada:** Encoder-Decoder\n",
        "\n",
        "**Dataset Link:** V1: [D-Talk](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatefr_to_pt) from TensorFlow.Datasets\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0iu8uQIc-P33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Projeto para traduzir mensagens do FrancÃªs para o Portugues**  \n",
        "\n",
        "Neste projeto, vamos explorar e comparar trÃªs arquiteturas de redes neurais para traduÃ§Ã£o automÃ¡tica do francÃªs para o portuguÃªs, usando o *dataset* TED Talks do *Open Translation Project*. A ideia Ã© testar modelos do tipo **Encoder-Decoder**, analisando suas diferenÃ§as e impacto na qualidade da traduÃ§Ã£o.  \n",
        "\n",
        "Os trÃªs modelos que vamos treinar sÃ£o:  \n",
        "\n",
        "1. **LSTM (Long Short-Term Memory)**  \n",
        "   - Um modelo bÃ¡sico de rede recorrente bidirecional. O **Encoder** processa a frase em francÃªs e gera um contexto, enquanto o **Decoder** usa esse contexto para formar a traduÃ§Ã£o em portuguÃªs.  \n",
        "   - A principal vantagem desse modelo Ã© sua capacidade de lidar com dependÃªncias de longo prazo nas sequÃªncias.  \n",
        "\n",
        "2. **LSTM com Mecanismos de AtenÃ§Ã£o**  \n",
        "   - Uma versÃ£o aprimorada do modelo anterior, adicionando camadas de atenÃ§Ã£o (produto escalar, Bahdanau e Luong).  \n",
        "   - A atenÃ§Ã£o ajuda o modelo a \"olhar\" para partes especÃ­ficas da frase de entrada enquanto traduz, melhorando a coerÃªncia e precisÃ£o.  \n",
        "\n",
        "3. **Transformers**  \n",
        "   - Uma abordagem mais moderna, baseada em **autoatenÃ§Ã£o**, eliminando o uso de redes recorrentes.  \n",
        "   - Trabalha com processamento paralelo, usando *Multi-Head Attention* e *Positional Encoding* para entender relaÃ§Ãµes entre palavras, mesmo quando estÃ£o distantes na frase.  \n",
        "\n",
        "**Como vamos testar os modelos?**  \n",
        "- **Dataset**: Vamos usar cerca de 52.000 pares de frases (francÃªs-portuguÃªs) para treinar, alÃ©m de 1.200 para validaÃ§Ã£o e 1.800 para teste.  \n",
        "- **PrÃ©-processamento**: Faremos a tokenizaÃ§Ã£o com *SubwordTextEncoder* para reduzir palavras fora do vocabulÃ¡rio (*out-of-vocabulary* â€“ OOV).  \n",
        "- **Treinamento**: OtimizaÃ§Ã£o com Adam, acompanhando a perda (*loss*) e a acurÃ¡cia durante o processo.  \n",
        "- **AvaliaÃ§Ã£o**: Vamos comparar os resultados usando a mÃ©trica BLEU e analisar exemplos prÃ¡ticos das traduÃ§Ãµes.  \n",
        "\n",
        "**O que esperamos encontrar?**  \n",
        "- Nosso objetivo Ã© entender qual desses modelos tem o melhor equilÃ­brio entre qualidade de traduÃ§Ã£o e eficiÃªncia computacional.  \n",
        "- Ã‰ provÃ¡vel que os Transformers tenham um desempenho superior, jÃ¡ que conseguem processar frases de forma mais eficiente, enquanto os modelos com LSTM e atenÃ§Ã£o devem mostrar um avanÃ§o significativo sobre a versÃ£o bÃ¡sica de LSTM.  \n",
        "\n",
        "No fim das contas, essa anÃ¡lise pode ajudar a compreender melhor como diferentes abordagens de deep learning se saem em tarefas de traduÃ§Ã£o, trazendo insights Ãºteis para aplicaÃ§Ãµes reais em NLP.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qqx0oHhH_YkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š Importando as bibliotecas necessÃ¡rias"
      ],
      "metadata": {
        "id": "rO-pcdsZGM-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o1RAVeffMKsD",
        "outputId": "70dee151-a714-40ad-80ad-a0cb9d1b9543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Primeiro, vamos importar todas as bibliotecas que vamos precisar ao longo do Projeto\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Para processamento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Componentes do Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, MultiHeadAttention, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Para visualizaÃ§Ã£o dos resultados\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tf.config.optimizer.set_jit(True)  # Ativa o XLA JIT compilation\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16') # ForÃ§a o TensorFlow a usar precisÃ£o mista\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ” Carregando e Preparando os Dados"
      ],
      "metadata": {
        "id": "epn56848Le9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/fr_to_pt', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGQjWQARLi89",
        "outputId": "34f22e00-a45f-4d3c-cc9e-9af67d296825"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataset carregado e preprocessado com sucesso! ğŸ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tf.strings.regex_replace(text, r\"([?.!,Â¿])\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r'[\" \"]+', \" \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "oDN1N2pQnAct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(examples, max_samples=None):\n",
        "    fr_texts, pt_texts = [], []\n",
        "    for fr, pt in examples:\n",
        "        fr_texts.append(preprocess_text(fr).numpy().decode('utf-8'))\n",
        "        pt_texts.append(preprocess_text(pt).numpy().decode('utf-8'))\n",
        "        if max_samples and len(fr_texts) >= max_samples:\n",
        "            break\n",
        "    return fr_texts, pt_texts"
      ],
      "metadata": {
        "id": "grtXEGGOm-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fr_texts, pt_texts = prepare_dataset(train_examples, max_samples=50000)"
      ],
      "metadata": {
        "id": "qtcSYCjmneb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_fr_texts, val_pt_texts = prepare_dataset(val_examples, max_samples=10000)"
      ],
      "metadata": {
        "id": "40mlJIMsnrGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ Configurando os Tokenizers"
      ],
      "metadata": {
        "id": "BlMfvl6wMzjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(fr_texts, pt_texts, max_input_length, max_target_length):\n",
        "    tokenizer_fr = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_fr.fit_on_texts(fr_texts)\n",
        "    input_vocab_size = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "    tokenizer_pt = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_pt.fit_on_texts(pt_texts)\n",
        "    target_vocab_size = len(tokenizer_pt.word_index) + 1\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    fr_sequences = tokenizer_fr.texts_to_sequences(fr_texts)\n",
        "    pt_sequences = tokenizer_pt.texts_to_sequences(pt_texts)\n",
        "\n",
        "    # Prepare decoder inputs and outputs\n",
        "    decoder_inputs = [seq[:-1] for seq in pt_sequences]\n",
        "    decoder_outputs = [seq[1:] for seq in pt_sequences]\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_inputs = pad_sequences(fr_sequences, maxlen=max_input_length, padding='post')\n",
        "    decoder_inputs = pad_sequences(decoder_inputs, maxlen=(max_target_length - 1), padding='post')\n",
        "    decoder_outputs = pad_sequences(decoder_outputs, maxlen=(max_target_length - 1), padding='post')\n",
        "\n",
        "    return encoder_inputs, decoder_inputs, decoder_outputs, input_vocab_size, target_vocab_size\n"
      ],
      "metadata": {
        "id": "2dHvGPlSfaaX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 50\n",
        "max_target_length = 50"
      ],
      "metadata": {
        "id": "yBXIIu7XqYEh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_inputs, train_decoder_inputs, train_decoder_outputs, input_vocab_size, target_vocab_size = tokenize_and_pad(\n",
        "    fr_texts, pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "tqLUNrnaqOgZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encoder_inputs, val_decoder_inputs, val_decoder_outputs, _, _ = tokenize_and_pad(\n",
        "    val_fr_texts, val_pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "esAVoW4ln0oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer_pt.word_index.get('[START]')"
      ],
      "metadata": {
        "id": "aGxzDq-R94xz"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_fr_sequences = tokenizer_fr.texts_to_sequences(fr_texts)\n",
        "#train_pt_sequences = tokenizer_pt.texts_to_sequences(pt_texts)"
      ],
      "metadata": {
        "id": "VIGkUTcdqMd6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleciona algumas amostras do dataset de treino prÃ©-processado\n",
        "#for i, (fr, pt) in enumerate(train_dataset.take(3)):\n",
        "#    print(f\"\\nAmostra {i+1}:\")\n",
        "#    print(\"FrancÃªs:  \", fr.numpy().decode('utf-8'))\n",
        "#    print(\"PortuguÃªs:\", pt.numpy().decode('utf-8'))\n",
        "#\n",
        "#val_fr_texts, val_pt_texts = [], []\n",
        "#for fr, pt in val_dataset.take(MAX_SAMPLES):\n",
        "#    val_fr_texts.append(fr.numpy().decode('utf-8'))\n",
        "#    val_pt_texts.append(pt.numpy().decode('utf-8'))\n",
        "#\n",
        "#val_fr_sequences = tokenizer_fr.texts_to_sequences(val_fr_texts)\n",
        "#val_pt_sequences = tokenizer_pt.texts_to_sequences(val_pt_texts)\n"
      ],
      "metadata": {
        "id": "a8ZRULeI1Hq0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#max_input_length = max(len(seq) for seq in train_fr_sequences)\n",
        "#max_target_length = max(len(seq) for seq in train_pt_sequences)"
      ],
      "metadata": {
        "id": "z7nvo5BnM0Wg"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_decoder_inputs = [seq[:-1] for seq in train_pt_sequences]\n",
        "#train_decoder_outputs = [seq[1:] for seq in train_pt_sequences]\n",
        "#\n",
        "#val_decoder_inputs = [seq[:-1] for seq in val_pt_sequences]\n",
        "#val_decoder_outputs = [seq[1:] for seq in val_pt_sequences]"
      ],
      "metadata": {
        "id": "PoM1bXiMqBdO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SequÃªncias pad\n",
        "#train_encoder_inputs = pad_sequences(train_fr_sequences, maxlen=max_input_length, padding='post')\n",
        "#train_decoder_inputs = pad_sequences(train_decoder_inputs, maxlen=(max_target_length-1), padding='post')\n",
        "#train_decoder_outputs = pad_sequences(train_decoder_outputs, maxlen=(max_target_length-1), padding='post')\n",
        "#\n",
        "#val_encoder_inputs = pad_sequences(val_fr_sequences, maxlen=max_input_length, padding='post')\n",
        "#val_decoder_inputs = pad_sequences(val_decoder_inputs, maxlen=(max_target_length-1), padding='post')\n",
        "#val_decoder_outputs = pad_sequences(val_decoder_outputs, maxlen=(max_target_length-1), padding='post')"
      ],
      "metadata": {
        "id": "KH9Sipk16w4O"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Modelo 1: LSTM BÃ¡sico\n",
        "### Vamos comeÃ§ar com o modelo mais simples: um **Encoder-Decoder** usando **LSTM**"
      ],
      "metadata": {
        "id": "Efz62L4hU--R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_vocab_size, target_vocab_size,\n",
        "                     max_input_len, max_target_len,\n",
        "                     latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,), name=\"encoder_inputs\")\n",
        "    encoder_embed = Embedding(input_vocab_size, embedding_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_state=True, name=\"encoder_lstm\")\n",
        "    _, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "\n",
        "    # Decoder (teacher forcing during training)\n",
        "    decoder_inputs = Input(shape=(max_target_len-1,), name=\"decoder_inputs\")\n",
        "    decoder_embed = Embedding(target_vocab_size, embedding_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True, name=\"decoder_lstm\")\n",
        "    decoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32', name=\"decoder_dense\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Build the full model and attach useful layers for inference.\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.encoder_inputs = encoder_inputs\n",
        "    model.encoder_embedding = model.get_layer(\"encoder_embedding\")\n",
        "    model.encoder_lstm = model.get_layer(\"encoder_lstm\")\n",
        "    model.decoder_embedding = model.get_layer(\"decoder_embedding\")\n",
        "    model.decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
        "    model.decoder_dense = decoder_dense\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "M6uiZBc2WLQF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando e Compilando o Modelo"
      ],
      "metadata": {
        "id": "O-GvW_reWVTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = build_lstm_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_input_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6eN7iyFZWXkf",
        "outputId": "2d06b030-da6e-403c-ad0e-ddfcc6130541"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_inputs            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_inputs            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_embedding         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m9,464,576\u001b[0m â”‚ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_embedding         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m9,603,328\u001b[0m â”‚ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   â”‚        \u001b[38;5;34m525,312\u001b[0m â”‚ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
              "â”‚                           â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚        \u001b[38;5;34m525,312\u001b[0m â”‚ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ cast_2 (\u001b[38;5;33mCast\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m37513\u001b[0m)     â”‚      \u001b[38;5;34m9,640,841\u001b[0m â”‚ cast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_inputs            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_inputs            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_embedding         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,464,576</span> â”‚ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_embedding         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,603,328</span> â”‚ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
              "â”‚                           â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37513</span>)     â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,640,841</span> â”‚ cast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Modelo"
      ],
      "metadata": {
        "id": "_4MEPbe8Wb9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "history = lstm_model.fit(\n",
        "    [train_enc, train_dec_in], train_dec_out,\n",
        "    validation_data=prepare_datasets(fr_tokenizer, pt_tokenizer, val_dataset, MAX_SAMPLES)[0],\n",
        "    #[train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\n",
        "    #validation_data=([val_encoder_inputs, val_decoder_inputs], val_decoder_outputs),\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "vfhCfnnHWe7v",
        "outputId": "24485f9b-059f-4e28-954f-edaf0fd7cd0d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4388/4388\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8836 - loss: 1.8984"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-674e7d1cc367>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = lstm_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#[train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Modelo 2: LSTM (Luong)\n"
      ],
      "metadata": {
        "id": "yNXz1Ebijgla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query = tf.expand_dims(query, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W(query + values)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "P92Vd5FsjfaL"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_luong_attention_model(input_vocab_size, target_vocab_size, max_source_len, max_target_len,\n",
        "                                    latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_source_len,))\n",
        "    enc_emb = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_sequences=True, return_state=True)\n",
        "    enc_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "    # Decoder with Attention\n",
        "    decoder_inputs = Input(shape=(max_target_len-1,))\n",
        "    dec_emb = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True, return_state=True)\n",
        "    dec_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    attention = LuongAttention(latent_units)\n",
        "    context_vector, _ = attention(dec_outputs, enc_outputs)\n",
        "    dec_outputs = tf.concat([context_vector, dec_outputs], axis=-1)\n",
        "\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32')\n",
        "    outputs = decoder_dense(dec_outputs)\n",
        "\n",
        "    return Model([encoder_inputs, decoder_inputs], outputs)"
      ],
      "metadata": {
        "id": "52gXid9Uk7kq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model = build_lstm_luong_attention_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_source_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "# optimizer=Adam(learning_rate=0.001)\n",
        "lstm_attn_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_attn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "dePkqj9kl2OZ",
        "outputId": "a5039c06-c3f7-4d71-ca5f-7eed5d0380f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'luong_attention' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Dimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'luong_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  â€¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  â€¢ kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8865d15aeb9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lstm_attn_model = build_lstm_luong_attention_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_source_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_target_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-c8788a8483d6>\u001b[0m in \u001b[0;36mbuild_lstm_luong_attention_model\u001b[0;34m(input_vocab_size, target_vocab_size, max_source_len, max_target_len, latent_units, embedding_dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLuongAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdec_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-ceb4c796fc11>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, query, values)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  â€¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  â€¢ kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model.fit(\n",
        "    [train_enc, train_dec_in], train_dec_out,\n",
        "    validation_data=prepare_datasets(fr_tokenizer, pt_tokenizer, val_dataset, MAX_SAMPLES)[0],\n",
        "    # validation_split=0.2,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "FHfBYEr1aGzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Š Visualizando os Resultados"
      ],
      "metadata": {
        "id": "im8JfjrfWjMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot da loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='ValidaÃ§Ã£o')\n",
        "    plt.title('Loss ao Longo do Treinamento')\n",
        "    plt.xlabel('Ã‰poca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot da acurÃ¡cia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='ValidaÃ§Ã£o')\n",
        "    plt.title('AcurÃ¡cia ao Longo do Treinamento')\n",
        "    plt.xlabel('Ã‰poca')\n",
        "    plt.ylabel('AcurÃ¡cia')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "70HJqHy1Wl86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_by_position(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plots the token-level accuracy at each sequence position.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the ground truth token indices.\n",
        "      - y_pred: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the predicted token indices.\n",
        "    \"\"\"\n",
        "    seq_length = y_true.shape[1]\n",
        "    accuracies = []\n",
        "    for pos in range(seq_length):\n",
        "        # Compute the fraction of tokens predicted correctly at this position.\n",
        "        pos_accuracy = np.mean(y_true[:, pos] == y_pred[:, pos])\n",
        "        accuracies.append(pos_accuracy)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(seq_length), accuracies, marker='o')\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy by Token Position\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Y-E3-rAeZ41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_length_distribution(y_true, y_pred):\n",
        "    # Calculando comprimentos (ignorando padding)\n",
        "    true_lengths = [len([x for x in seq if x != 0]) for seq in y_true]\n",
        "    pred_lengths = [len([x for x in seq if x != 0]) for seq in y_pred]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist([true_lengths, pred_lengths], label=['Real', 'Previsto'],\n",
        "             alpha=0.7, bins=20)\n",
        "    plt.title('DistribuiÃ§Ã£o do Comprimento das TraduÃ§Ãµes')\n",
        "    plt.xlabel('Comprimento da SequÃªncia')\n",
        "    plt.ylabel('FrequÃªncia')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8tlGUuaWb1i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_heatmap(y_true, y_pred, token_labels):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix (as a heatmap) for token predictions.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of token indices (can be 2D or flattened)\n",
        "      - y_pred: numpy array of token indices (can be 2D or flattened)\n",
        "      - token_labels: list of strings that maps each token index to a label.\n",
        "                      For example: [\"PAD\", \"[START]\", \"[END]\", \"bonjour\", ...]\n",
        "    \"\"\"\n",
        "    # If the inputs are 2D (num_samples x seq_length), flatten them.\n",
        "    if y_true.ndim > 1:\n",
        "        y_true = y_true.flatten()\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # Compute the confusion matrix.\n",
        "    labels = np.arange(len(token_labels))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=token_labels,\n",
        "                yticklabels=token_labels)\n",
        "    plt.xlabel(\"Predicted Token\")\n",
        "    plt.ylabel(\"True Token\")\n",
        "    plt.title(\"Confusion Matrix of Token Predictions\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q7I9QRrpb32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, assume we have 100 samples, each of length 10 tokens.\n",
        "num_samples = 100\n",
        "sequence_length = 10\n",
        "vocab_size = 50  # For example, assume your vocabulary has 50 tokens (indices 0 to 49)\n",
        "\n",
        "# Create some random demo ground truth and predicted token sequences.\n",
        "np.random.seed(42)\n",
        "\n",
        "# To-do: real data here.\n",
        "y_true_demo = np.random.randint(0, vocab_size, size=(num_samples, sequence_length))\n",
        "y_pred_demo = np.random.randint(0, vocab_size, size=(num_samples, sequence_length))\n",
        "\n",
        "# Create a list of token labels for the confusion heatmap.\n",
        "# (In your case, you might use tokenizer_pt.index_word but here we simulate labels.)\n",
        "token_labels = [f\"Token {i}\" for i in range(vocab_size)]\n",
        "\n",
        "# Plot accuracy by token position.\n",
        "plot_accuracy_by_position(y_true_demo, y_pred_demo)\n",
        "\n",
        "# Plot confusion heatmap.\n",
        "plot_confusion_heatmap(y_true_demo, y_pred_demo, token_labels)"
      ],
      "metadata": {
        "id": "Vg1MAdWxZ-UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. AcurÃ¡cia por posiÃ§Ã£o\n",
        "val_data = [train_encoder_inputs[-1000:], train_decoder_inputs[-1000:]]  # Usando Ãºltimas 1000 amostras como validaÃ§Ã£o\n",
        "plot_accuracy_by_position(model, val_data[0], val_data[1])\n",
        "\n",
        "# 2. DistribuiÃ§Ã£o de comprimentos\n",
        "predictions = model.predict([val_data[0], val_data[1][:, :-1]])\n",
        "pred_classes = np.argmax(predictions, axis=-1)\n",
        "plot_length_distribution(val_data[1][:, 1:], pred_classes)\n",
        "\n",
        "# 3. Heatmap de confusÃ£o\n",
        "plot_confusion_heatmap(val_data[1][:, 1:], pred_classes, tokenizer_pt)\n",
        "\n",
        "print(\"\\nLegenda das visualizaÃ§Ãµes:\")\n",
        "print(\"1. GrÃ¡fico de AcurÃ¡cia por PosiÃ§Ã£o: Mostra como o modelo se comporta em diferentes posiÃ§Ãµes da sequÃªncia\")\n",
        "print(\"2. DistribuiÃ§Ã£o de Comprimentos: Compara o tamanho das traduÃ§Ãµes reais vs. previstas\")\n",
        "print(\"3. Heatmap: Mostra quais palavras frequentes sÃ£o mais confundidas entre si\")"
      ],
      "metadata": {
        "id": "seE3fGeabeMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}