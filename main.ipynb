{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Trabalho 1 - T√≥picos Especiais em Matem√°tica Aplicada\n",
        "\n",
        "**Alunos/Matricula:** Jo√£o V. Farias & Renan V. Guedes / 221022604 & 221031363\n",
        "\n",
        "**Arquitetura Usada:** Encoder-Decoder\n",
        "\n",
        "**Dataset Link:** V1: [D-Talk](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatefr_to_pt) from TensorFlow.Datasets\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0iu8uQIc-P33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Projeto para traduzir mensagens do Franc√™s para o Portugues**  \n",
        "\n",
        "Neste projeto, vamos explorar e comparar tr√™s arquiteturas de redes neurais para tradu√ß√£o autom√°tica do franc√™s para o portugu√™s, usando o *dataset* TED Talks do *Open Translation Project*. A ideia √© testar modelos do tipo **Encoder-Decoder**, analisando suas diferen√ßas e impacto na qualidade da tradu√ß√£o.  \n",
        "\n",
        "Os tr√™s modelos que vamos treinar s√£o:  \n",
        "\n",
        "1. **LSTM (Long Short-Term Memory)**  \n",
        "   - Um modelo b√°sico de rede recorrente bidirecional. O **Encoder** processa a frase em franc√™s e gera um contexto, enquanto o **Decoder** usa esse contexto para formar a tradu√ß√£o em portugu√™s.  \n",
        "   - A principal vantagem desse modelo √© sua capacidade de lidar com depend√™ncias de longo prazo nas sequ√™ncias.  \n",
        "\n",
        "2. **LSTM com Mecanismos de Aten√ß√£o**  \n",
        "   - Uma vers√£o aprimorada do modelo anterior, adicionando uma camada de aten√ß√£o (no nosso caso, escolhemos Luong).\n",
        "   - A aten√ß√£o ajuda o modelo a \"olhar\" para partes espec√≠ficas da frase de entrada enquanto traduz, melhorando a coer√™ncia e precis√£o.  \n",
        "\n",
        "3. **Transformers**  \n",
        "   - Uma abordagem mais moderna, baseada em **autoaten√ß√£o**, tirando o uso uso de redes recorrentes.  \n",
        "   - Trabalha com processamento paralelo, usando *Multi-Head Attention* e *Positional Encoding* para entender rela√ß√µes entre palavras, mesmo quando est√£o distantes na frase.  \n",
        "\n",
        "**Como vamos testar os modelos?**  \n",
        "- **Dataset**: Vamos usar cerca de 52.000 pares de frases (franc√™s-portugu√™s) para treinar, al√©m de 1.200 para valida√ß√£o e 1.800 para teste.  \n",
        "- **Pr√©-processamento**: Faremos a tokeniza√ß√£o com *SubwordTextEncoder* para reduzir palavras fora do vocabul√°rio (*out-of-vocabulary* ‚Äì OOV).  \n",
        "- **Treinamento**: Otimiza√ß√£o com Adam, acompanhando o loss e a acur√°cia durante o processo.  \n",
        "- **Avalia√ß√£o**: Vamos comparar os resultados usando a m√©trica BLEU e analisar exemplos pr√°ticos das tradu√ß√µes.  \n",
        "\n",
        "**O que esperamos encontrar?**  \n",
        "- entender qual desses modelos tem o melhor equil√≠brio entre qualidade de tradu√ß√£o e efici√™ncia computacional.\n",
        "- prov√°vel que o transformer tenha um desempenho superior, j√° que conseguem processar frases de forma mais eficiente, enquanto o modelo com LSTM e aten√ß√£o deve mostrar um avan√ßo significativo sobre a vers√£o b√°sica de LSTM.\n",
        "\n",
        "No fim das contas, tentamos compreender melhor como essas diferentes t√©cnicas v√£o trazer como resultado.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qqx0oHhH_YkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Importando as bibliotecas necess√°rias"
      ],
      "metadata": {
        "id": "rO-pcdsZGM-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o1RAVeffMKsD",
        "outputId": "70dee151-a714-40ad-80ad-a0cb9d1b9543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Primeiro, vamos importar todas as bibliotecas que vamos precisar ao longo do Projeto\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Para processamento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Componentes do Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, MultiHeadAttention, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Para visualiza√ß√£o dos resultados\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tf.config.optimizer.set_jit(True)  # Ativa o XLA JIT compilation\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16') # For√ßa o TensorFlow a usar precis√£o mista\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Carregando e Preparando os Dados"
      ],
      "metadata": {
        "id": "epn56848Le9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/fr_to_pt', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGQjWQARLi89",
        "outputId": "34f22e00-a45f-4d3c-cc9e-9af67d296825"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataset carregado e preprocessado com sucesso! üéâ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A gente come√ßou limpando os textos, retirando aqueles s√≠mbolos tipo ?, !, . e at√© o ¬ø, que no franc√™s e no portugu√™s n√£o s√£o usados do mesmo jeito. Esses sinais tamb√©m atrapalham na hora de treinar, ent√£o decidimos remov√™-los pra deixar tudo mais uniforme e f√°cil de lidar.."
      ],
      "metadata": {
        "id": "gBpV63k6ww1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tf.strings.regex_replace(text, r\"([?.!,¬ø])\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r'[\" \"]+', \" \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "oDN1N2pQnAct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ideia aqui foi organizar os dados de forma que a gente conseguisse trabalhar numa boa com eles. Separamos as frases e garantimos que cada exemplo estivesse no formato certo. Foi interessante ver como uma boa prepara√ß√£o dos dados j√° facilita bastante o treino dos modelos depois."
      ],
      "metadata": {
        "id": "nP_8edELw7su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(examples, max_samples=None):\n",
        "    fr_texts, pt_texts = [], []\n",
        "    for fr, pt in examples:\n",
        "        fr_texts.append(preprocess_text(fr).numpy().decode('utf-8'))\n",
        "        pt_texts.append(preprocess_text(pt).numpy().decode('utf-8'))\n",
        "        if max_samples and len(fr_texts) >= max_samples:\n",
        "            break\n",
        "    return fr_texts, pt_texts"
      ],
      "metadata": {
        "id": "grtXEGGOm-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pra n√£o sobrecarregar o sistema e tamb√©m evitar overfitting, a gente pegou 50 mil amostras pra treinamento e 10 mil pra valida√ß√£o."
      ],
      "metadata": {
        "id": "2gXoI1YDxBaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fr_texts, pt_texts = prepare_dataset(train_examples, max_samples=50000)"
      ],
      "metadata": {
        "id": "qtcSYCjmneb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_fr_texts, val_pt_texts = prepare_dataset(val_examples, max_samples=10000)"
      ],
      "metadata": {
        "id": "40mlJIMsnrGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Configurando os Tokenizers"
      ],
      "metadata": {
        "id": "BlMfvl6wMzjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criamos uma fun√ß√£o que faz a tokeniza√ß√£o e o padding das frases. Usamos o token `[OOV]` pra lidar com palavras que n√£o est√£o no vocabul√°rio e, depois, aplicamos padding pra deixar todas as sequ√™ncias com o mesmo tamanho. Assim, fica mais f√°cil converter o texto em sequ√™ncias num√©ricas e alimentar a rede neural sem complica√ß√£o."
      ],
      "metadata": {
        "id": "R27poN0ExMZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(fr_texts, pt_texts, max_input_length, max_target_length):\n",
        "    tokenizer_fr = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_fr.fit_on_texts(fr_texts)\n",
        "    input_vocab_size = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "    tokenizer_pt = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_pt.fit_on_texts(pt_texts)\n",
        "    target_vocab_size = len(tokenizer_pt.word_index) + 1\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    fr_sequences = tokenizer_fr.texts_to_sequences(fr_texts)\n",
        "    pt_sequences = tokenizer_pt.texts_to_sequences(pt_texts)\n",
        "\n",
        "    # Prepare decoder inputs and outputs\n",
        "    decoder_inputs = [seq[:-1] for seq in pt_sequences]\n",
        "    decoder_outputs = [seq[1:] for seq in pt_sequences]\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_inputs = pad_sequences(fr_sequences, maxlen=max_input_length, padding='post')\n",
        "    decoder_inputs = pad_sequences(decoder_inputs, maxlen=(max_target_length - 1), padding='post')\n",
        "    decoder_outputs = pad_sequences(decoder_outputs, maxlen=(max_target_length - 1), padding='post')\n",
        "\n",
        "    return encoder_inputs, decoder_inputs, decoder_outputs, input_vocab_size, target_vocab_size\n"
      ],
      "metadata": {
        "id": "2dHvGPlSfaaX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 50\n",
        "max_target_length = 50"
      ],
      "metadata": {
        "id": "yBXIIu7XqYEh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_inputs, train_decoder_inputs, train_decoder_outputs, input_vocab_size, target_vocab_size = tokenize_and_pad(\n",
        "    fr_texts, pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "tqLUNrnaqOgZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encoder_inputs, val_decoder_inputs, val_decoder_outputs, _, _ = tokenize_and_pad(\n",
        "    val_fr_texts, val_pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "esAVoW4ln0oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Modelo 1: LSTM B√°sico\n",
        "\n",
        "No modelo LSTM, definimos a fun√ß√£o build_lstm_model que basicamente cria um encoder e um decoder usando teacher forcing. A ideia √© que o encoder pegue a sequ√™ncia de entrada e o decoder aprenda a gerar a sa√≠da com base nas informa√ß√µes que recebeu, sempre ‚Äúcorrigindo‚Äù o que j√° foi gerado. As camadas de infer√™ncia, que seriam usadas na hora de testar ou gerar tradu√ß√µes de verdade, a gente deixou de lado, mas o ChatGPT e o Deepseek recomendaram para gente, mas n√£o era o foco do trabalho."
      ],
      "metadata": {
        "id": "Efz62L4hU--R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_vocab_size, target_vocab_size,\n",
        "                     max_input_len, max_target_len,\n",
        "                     latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,), name=\"encoder_inputs\")\n",
        "    encoder_embed = Embedding(input_vocab_size, embedding_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_state=True, name=\"encoder_lstm\")\n",
        "    _, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len-1,), name=\"decoder_inputs\")\n",
        "    decoder_embed = Embedding(target_vocab_size, embedding_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True, name=\"decoder_lstm\")\n",
        "    decoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32', name=\"decoder_dense\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.encoder_inputs = encoder_inputs\n",
        "    model.encoder_embedding = model.get_layer(\"encoder_embedding\")\n",
        "    model.encoder_lstm = model.get_layer(\"encoder_lstm\")\n",
        "    model.decoder_embedding = model.get_layer(\"decoder_embedding\")\n",
        "    model.decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
        "    model.decoder_dense = decoder_dense\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "M6uiZBc2WLQF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando e Compilando os modelos\n",
        "\n",
        "Para todos os nossos modelos, a configura√ß√£o foi padr√£o: usamos o otimizador Adam com learning rate de 1e-4 e clipnorm de 1.0, a loss foi definida como 'sparse_categorical_crossentropy' e a m√©trica de acur√°cia. Essa padroniza√ß√£o ajudou a gente a comparar os resultados dos modelos sem ter que ficar ajustando v√°rios par√¢metros diferentes."
      ],
      "metadata": {
        "id": "O-GvW_reWVTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = build_lstm_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_input_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6eN7iyFZWXkf",
        "outputId": "2d06b030-da6e-403c-ad0e-ddfcc6130541"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ encoder_inputs            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m)            ‚îÇ              \u001b[38;5;34m0\u001b[0m ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_inputs            ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            ‚îÇ              \u001b[38;5;34m0\u001b[0m ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mInputLayer\u001b[0m)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_embedding         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ      \u001b[38;5;34m9,464,576\u001b[0m ‚îÇ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mEmbedding\u001b[0m)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_embedding         ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ      \u001b[38;5;34m9,603,328\u001b[0m ‚îÇ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   ‚îÇ\n",
              "‚îÇ (\u001b[38;5;33mEmbedding\u001b[0m)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   ‚îÇ        \u001b[38;5;34m525,312\u001b[0m ‚îÇ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
              "‚îÇ                           ‚îÇ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ        \u001b[38;5;34m525,312\u001b[0m ‚îÇ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m‚Ä¶\u001b[0m ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ cast_2 (\u001b[38;5;33mCast\u001b[0m)             ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       ‚îÇ              \u001b[38;5;34m0\u001b[0m ‚îÇ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m37513\u001b[0m)     ‚îÇ      \u001b[38;5;34m9,640,841\u001b[0m ‚îÇ cast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
              "‚îÉ<span style=\"font-weight: bold\"> Layer (type)              </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape           </span>‚îÉ<span style=\"font-weight: bold\">        Param # </span>‚îÉ<span style=\"font-weight: bold\"> Connected to           </span>‚îÉ\n",
              "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
              "‚îÇ encoder_inputs            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>)            ‚îÇ              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_inputs            ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            ‚îÇ              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ -                      ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_embedding         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,464,576</span> ‚îÇ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_embedding         ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,603,328</span> ‚îÇ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   ‚îÇ\n",
              "‚îÇ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               ‚îÇ                        ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> ‚îÇ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
              "‚îÇ                           ‚îÇ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     ‚îÇ                ‚îÇ                        ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> ‚îÇ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">‚Ä¶</span> ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    ‚îÇ\n",
              "‚îÇ                           ‚îÇ                        ‚îÇ                ‚îÇ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       ‚îÇ              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     ‚îÇ\n",
              "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
              "‚îÇ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37513</span>)     ‚îÇ      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,640,841</span> ‚îÇ cast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           ‚îÇ\n",
              "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Modelo"
      ],
      "metadata": {
        "id": "_4MEPbe8Wb9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "history = lstm_model.fit(\n",
        "    [train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\n",
        "    validation_data=([val_encoder_inputs, val_decoder_inputs], val_decoder_outputs),\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "vfhCfnnHWe7v",
        "outputId": "24485f9b-059f-4e28-954f-edaf0fd7cd0d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4388/4388\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8836 - loss: 1.8984"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-674e7d1cc367>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = lstm_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#[train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Modelo 2: LSTM (Luong)\n",
        "\n",
        "A gente adaptou o modelo LSTM para incluir um mecanismo de aten√ß√£o ‚Äì escolhemos o esquema de Luong mesmo, pois pareceu mais simples e direto. Aqui, em vez de usar teacher forcing, o decoder ‚Äúpresta aten√ß√£o‚Äù nas partes relevantes do encoder durante a gera√ß√£o da tradu√ß√£o. Foi interessante notar como a aten√ß√£o ajudou a melhorar o alinhamento entre as palavras de entrada e sa√≠da, mesmo com uma estrutura parecida com a LSTM simples.\n"
      ],
      "metadata": {
        "id": "yNXz1Ebijgla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_attention_model(input_vocab_size, target_vocab_size, max_input_len, max_target_len, latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,))\n",
        "    encoder_embed = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len - 1,))\n",
        "    decoder_embed = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True)\n",
        "    decoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Attention\n",
        "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "    decoder_concat = tf.concat([decoder_outputs, attention], axis=-1)\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32')\n",
        "    decoder_outputs = decoder_dense(decoder_concat)\n",
        "\n",
        "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "52gXid9Uk7kq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model = build_lstm_attention_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_input_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "# optimizer=Adam(learning_rate=0.001)\n",
        "lstm_attn_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_attn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "dePkqj9kl2OZ",
        "outputId": "a5039c06-c3f7-4d71-ca5f-7eed5d0380f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'luong_attention' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Dimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'luong_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  ‚Ä¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  ‚Ä¢ kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8865d15aeb9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lstm_attn_model = build_lstm_luong_attention_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_source_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_target_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-c8788a8483d6>\u001b[0m in \u001b[0;36mbuild_lstm_luong_attention_model\u001b[0;34m(input_vocab_size, target_vocab_size, max_source_len, max_target_len, latent_units, embedding_dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLuongAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdec_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-ceb4c796fc11>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, query, values)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  ‚Ä¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  ‚Ä¢ kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model.fit(\n",
        "    [train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\n",
        "    validation_data=([val_encoder_inputs, val_decoder_inputs], val_decoder_outputs),\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "FHfBYEr1aGzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Modelo 3: Transformador\n",
        "\n",
        "Por fim, implementamos um Transformer, que √© uma abordagem totalmente diferente, sem recorr√™ncia. No c√≥digo, o encoder e o decoder s√£o feitos de v√°rias camadas com MultiHeadAttention, normaliza√ß√£o e dropout. Cada embedding √© escalado e normalizado antes de entrar nas camadas, e no final temos uma camada densa com softmax pra prever as palavras. Esse modelo chamou nossa aten√ß√£o porque, apesar de mais complexo, mostrou um potencial √∫nico pra capturar rela√ß√µes mais profundas nas senten√ßas como o professor tinha dito em sala."
      ],
      "metadata": {
        "id": "I9Rs3XOzqqc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer_model(input_vocab_size, target_vocab_size, max_input_len, max_target_len, num_layers=2, d_model=256, num_heads=8, dff=512, dropout_rate=0.1):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,))\n",
        "    encoder_embed = Embedding(input_vocab_size, d_model)(encoder_inputs)\n",
        "    encoder_embed *= tf.math.sqrt(tf.cast(d_model, tf.float32))  # Scale embeddings\n",
        "    encoder_embed = LayerNormalization(epsilon=1e-6)(encoder_embed)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(encoder_embed, encoder_embed)\n",
        "        attn_output = Dropout(dropout_rate)(attn_output)\n",
        "        encoder_embed = LayerNormalization(epsilon=1e-6)(encoder_embed + attn_output)\n",
        "\n",
        "        ffn_output = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])(encoder_embed)\n",
        "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "        encoder_embed = LayerNormalization(epsilon=1e-6)(encoder_embed + ffn_output)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len - 1,))\n",
        "    decoder_embed = Embedding(target_vocab_size, d_model)(decoder_inputs)\n",
        "    decoder_embed *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    decoder_embed = LayerNormalization(epsilon=1e-6)(decoder_embed)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(decoder_embed, decoder_embed)\n",
        "        attn_output = Dropout(dropout_rate)(attn_output)\n",
        "        decoder_embed = LayerNormalization(epsilon=1e-6)(decoder_embed + attn_output)\n",
        "\n",
        "        ffn_output = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])(decoder_embed)\n",
        "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "        decoder_embed = LayerNormalization(epsilon=1e-6)(decoder_embed + ffn_output)\n",
        "\n",
        "    # Final output\n",
        "    decoder_outputs = Dense(target_vocab_size, activation='softmax', dtype='float32')(decoder_embed)\n",
        "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "z8SbO511quGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = build_transformer_model(input_vocab_size, target_vocab_size, max_input_length, max_target_length)\n",
        "\n",
        "# optimizer=Adam(learning_rate=0.001)\n",
        "transformer_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "transformer_model.summary()"
      ],
      "metadata": {
        "id": "ldrnwqt9q2XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Visualizando os Resultados"
      ],
      "metadata": {
        "id": "im8JfjrfWjMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot da loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='Valida√ß√£o')\n",
        "    plt.title('Loss ao Longo do Treinamento')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot da acur√°cia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='Valida√ß√£o')\n",
        "    plt.title('Acur√°cia ao Longo do Treinamento')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Acur√°cia')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "70HJqHy1Wl86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_by_position(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plots the token-level accuracy at each sequence position.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the ground truth token indices.\n",
        "      - y_pred: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the predicted token indices.\n",
        "    \"\"\"\n",
        "    seq_length = y_true.shape[1]\n",
        "    accuracies = []\n",
        "    for pos in range(seq_length):\n",
        "        # Compute the fraction of tokens predicted correctly at this position.\n",
        "        pos_accuracy = np.mean(y_true[:, pos] == y_pred[:, pos])\n",
        "        accuracies.append(pos_accuracy)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(seq_length), accuracies, marker='o')\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy by Token Position\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Y-E3-rAeZ41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_length_distribution(y_true, y_pred):\n",
        "    # Calculando comprimentos (ignorando padding)\n",
        "    true_lengths = [len([x for x in seq if x != 0]) for seq in y_true]\n",
        "    pred_lengths = [len([x for x in seq if x != 0]) for seq in y_pred]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist([true_lengths, pred_lengths], label=['Real', 'Previsto'],\n",
        "             alpha=0.7, bins=20)\n",
        "    plt.title('Distribui√ß√£o do Comprimento das Tradu√ß√µes')\n",
        "    plt.xlabel('Comprimento da Sequ√™ncia')\n",
        "    plt.ylabel('Frequ√™ncia')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8tlGUuaWb1i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_heatmap(y_true, y_pred, token_labels):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix (as a heatmap) for token predictions.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of token indices (can be 2D or flattened)\n",
        "      - y_pred: numpy array of token indices (can be 2D or flattened)\n",
        "      - token_labels: list of strings that maps each token index to a label.\n",
        "                      For example: [\"PAD\", \"[START]\", \"[END]\", \"bonjour\", ...]\n",
        "    \"\"\"\n",
        "    # If the inputs are 2D (num_samples x seq_length), flatten them.\n",
        "    if y_true.ndim > 1:\n",
        "        y_true = y_true.flatten()\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # Compute the confusion matrix.\n",
        "    labels = np.arange(len(token_labels))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=token_labels,\n",
        "                yticklabels=token_labels)\n",
        "    plt.xlabel(\"Predicted Token\")\n",
        "    plt.ylabel(\"True Token\")\n",
        "    plt.title(\"Confusion Matrix of Token Predictions\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q7I9QRrpb32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate predictions\n",
        "def generate_predictions(model, encoder_inputs, decoder_inputs, max_target_length):\n",
        "    predictions = model.predict([encoder_inputs, decoder_inputs])\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    return predictions\n",
        "\n",
        "# Token labels for confusion matrix (excluding padding token 0)\n",
        "token_labels = list(range(1, target_vocab_size))\n",
        "\n",
        "models = {\n",
        "    \"LSTM B√°sico\": lstm_model,\n",
        "    \"LSTM (Luong)\": lstm_attn_model,\n",
        "    \"Transformer\": transformer_model\n",
        "}\n",
        "\n",
        "\n",
        "# Analyze each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nAnalyzing {name} model...\")\n",
        "\n",
        "    # Generate predictions\n",
        "    train_predictions = generate_predictions(model, train_encoder_inputs, train_decoder_inputs, max_target_length)\n",
        "    val_predictions = generate_predictions(model, val_encoder_inputs, val_decoder_inputs, max_target_length)\n",
        "\n",
        "    # Visualize training results\n",
        "    visualize_results(history)\n",
        "\n",
        "    # Plot accuracy by position\n",
        "    plot_accuracy_by_position(train_decoder_outputs, train_predictions)\n",
        "\n",
        "    # Plot sequence length distribution\n",
        "    plot_length_distribution(train_decoder_outputs, train_predictions)\n",
        "\n",
        "    # Plot confusion matrix heatmap\n",
        "    plot_confusion_heatmap(train_decoder_outputs, train_predictions, token_labels)"
      ],
      "metadata": {
        "id": "Vg1MAdWxZ-UV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}