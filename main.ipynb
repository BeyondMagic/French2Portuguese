{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Trabalho 1 - TÃ³picos Especiais em MatemÃ¡tica Aplicada\n",
        "\n",
        "**Alunos/Matricula:** JoÃ£o V. Farias & Renan V. Guedes / 221022604 & 221031363\n",
        "\n",
        "**Arquitetura Usada:** Encoder-Decoder\n",
        "\n",
        "**Dataset Link:** V1: [D-Talk](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatefr_to_pt) from TensorFlow.Datasets\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0iu8uQIc-P33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Projeto para traduzir mensagens do FrancÃªs para o Portugues**  \n",
        "\n",
        "Neste projeto, vamos explorar e comparar trÃªs arquiteturas de redes neurais para traduÃ§Ã£o automÃ¡tica do francÃªs para o portuguÃªs, usando o *dataset* TED Talks do *Open Translation Project*. A ideia Ã© testar modelos do tipo **Encoder-Decoder**, analisando suas diferenÃ§as e impacto na qualidade da traduÃ§Ã£o.  \n",
        "\n",
        "Os trÃªs modelos que vamos treinar sÃ£o:  \n",
        "\n",
        "1. **LSTM (Long Short-Term Memory)**  \n",
        "   - Um modelo bÃ¡sico de rede recorrente bidirecional. O **Encoder** processa a frase em francÃªs e gera um contexto, enquanto o **Decoder** usa esse contexto para formar a traduÃ§Ã£o em portuguÃªs.  \n",
        "   - A principal vantagem desse modelo Ã© sua capacidade de lidar com dependÃªncias de longo prazo nas sequÃªncias.  \n",
        "\n",
        "2. **LSTM com Mecanismos de AtenÃ§Ã£o**  \n",
        "   - Uma versÃ£o aprimorada do modelo anterior, adicionando uma camada de atenÃ§Ã£o (no nosso caso, escolhemos Luong).\n",
        "   - A atenÃ§Ã£o ajuda o modelo a \"olhar\" para partes especÃ­ficas da frase de entrada enquanto traduz, melhorando a coerÃªncia e precisÃ£o.  \n",
        "\n",
        "3. **Transformers**  \n",
        "   - Uma abordagem mais moderna, baseada em **autoatenÃ§Ã£o**, tirando o uso uso de redes recorrentes.  \n",
        "   - Trabalha com processamento paralelo, usando *Multi-Head Attention* e *Positional Encoding* para entender relaÃ§Ãµes entre palavras, mesmo quando estÃ£o distantes na frase.  \n",
        "\n",
        "**Como vamos testar os modelos?**  \n",
        "- **Dataset**: Vamos usar cerca de 52.000 pares de frases (francÃªs-portuguÃªs) para treinar, alÃ©m de 1.200 para validaÃ§Ã£o e 1.800 para teste.  \n",
        "- **PrÃ©-processamento**: Faremos a tokenizaÃ§Ã£o com *SubwordTextEncoder* para reduzir palavras fora do vocabulÃ¡rio (*out-of-vocabulary* â€“ OOV).  \n",
        "- **Treinamento**: OtimizaÃ§Ã£o com Adam, acompanhando o loss e a acurÃ¡cia durante o processo.  \n",
        "- **AvaliaÃ§Ã£o**: Vamos comparar os resultados usando a mÃ©trica BLEU e analisar exemplos prÃ¡ticos das traduÃ§Ãµes.  \n",
        "\n",
        "**O que esperamos encontrar?**  \n",
        "- entender qual desses modelos tem o melhor equilÃ­brio entre qualidade de traduÃ§Ã£o e eficiÃªncia computacional.\n",
        "- provÃ¡vel que o transformer tenha um desempenho superior, jÃ¡ que conseguem processar frases de forma mais eficiente, enquanto o modelo com LSTM e atenÃ§Ã£o deve mostrar um avanÃ§o significativo sobre a versÃ£o bÃ¡sica de LSTM.\n",
        "\n",
        "No fim das contas, tentamos compreender melhor como essas diferentes tÃ©cnicas vÃ£o trazer como resultado.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qqx0oHhH_YkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š Importando as bibliotecas necessÃ¡rias"
      ],
      "metadata": {
        "id": "rO-pcdsZGM-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o1RAVeffMKsD",
        "outputId": "70dee151-a714-40ad-80ad-a0cb9d1b9543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "# Primeiro, vamos importar todas as bibliotecas que vamos precisar ao longo do Projeto\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Para processamento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Componentes do Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, MultiHeadAttention, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Para visualizaÃ§Ã£o dos resultados\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "tf.config.optimizer.set_jit(True)  # Ativa o XLA JIT compilation\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16') # ForÃ§a o TensorFlow a usar precisÃ£o mista\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ” Carregando e Preparando os Dados"
      ],
      "metadata": {
        "id": "epn56848Le9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/fr_to_pt', with_info=True, as_supervised=True)\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGQjWQARLi89",
        "outputId": "34f22e00-a45f-4d3c-cc9e-9af67d296825"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataset carregado e preprocessado com sucesso! ğŸ‰\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A gente comeÃ§ou limpando os textos, retirando aqueles sÃ­mbolos tipo ?, !, . e atÃ© o Â¿, que no francÃªs e no portuguÃªs nÃ£o sÃ£o usados do mesmo jeito. Esses sinais tambÃ©m atrapalham na hora de treinar, entÃ£o decidimos removÃª-los pra deixar tudo mais uniforme e fÃ¡cil de lidar.."
      ],
      "metadata": {
        "id": "gBpV63k6ww1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tf.strings.regex_replace(text, r\"([?.!,Â¿])\", r\" \\1 \")\n",
        "    text = tf.strings.regex_replace(text, r'[\" \"]+', \" \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "oDN1N2pQnAct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ideia aqui foi organizar os dados de forma que a gente conseguisse trabalhar numa boa com eles. Separamos as frases e garantimos que cada exemplo estivesse no formato certo. Foi interessante ver como uma boa preparaÃ§Ã£o dos dados jÃ¡ facilita bastante o treino dos modelos depois."
      ],
      "metadata": {
        "id": "nP_8edELw7su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(examples, max_samples=None):\n",
        "    fr_texts, pt_texts = [], []\n",
        "    for fr, pt in examples:\n",
        "        fr_texts.append(preprocess_text(fr).numpy().decode('utf-8'))\n",
        "        pt_texts.append(preprocess_text(pt).numpy().decode('utf-8'))\n",
        "        if max_samples and len(fr_texts) >= max_samples:\n",
        "            break\n",
        "    return fr_texts, pt_texts"
      ],
      "metadata": {
        "id": "grtXEGGOm-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pra nÃ£o sobrecarregar o sistema e tambÃ©m evitar overfitting, a gente pegou 50 mil amostras pra treinamento e 10 mil pra validaÃ§Ã£o."
      ],
      "metadata": {
        "id": "2gXoI1YDxBaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fr_texts, pt_texts = prepare_dataset(train_examples, max_samples=50000)"
      ],
      "metadata": {
        "id": "qtcSYCjmneb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_fr_texts, val_pt_texts = prepare_dataset(val_examples, max_samples=10000)"
      ],
      "metadata": {
        "id": "40mlJIMsnrGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ Configurando os Tokenizers"
      ],
      "metadata": {
        "id": "BlMfvl6wMzjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criamos uma funÃ§Ã£o que faz a tokenizaÃ§Ã£o e o padding das frases. Usamos o token `[OOV]` pra lidar com palavras que nÃ£o estÃ£o no vocabulÃ¡rio e, depois, aplicamos padding pra deixar todas as sequÃªncias com o mesmo tamanho. Assim, fica mais fÃ¡cil converter o texto em sequÃªncias numÃ©ricas e alimentar a rede neural sem complicaÃ§Ã£o."
      ],
      "metadata": {
        "id": "R27poN0ExMZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_pad(fr_texts, pt_texts, max_input_length, max_target_length):\n",
        "    tokenizer_fr = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_fr.fit_on_texts(fr_texts)\n",
        "    input_vocab_size = len(tokenizer_fr.word_index) + 1\n",
        "\n",
        "    tokenizer_pt = Tokenizer(filters='', oov_token='[OOV]')\n",
        "    tokenizer_pt.fit_on_texts(pt_texts)\n",
        "    target_vocab_size = len(tokenizer_pt.word_index) + 1\n",
        "\n",
        "    # Convert texts to sequences\n",
        "    fr_sequences = tokenizer_fr.texts_to_sequences(fr_texts)\n",
        "    pt_sequences = tokenizer_pt.texts_to_sequences(pt_texts)\n",
        "\n",
        "    # Prepare decoder inputs and outputs\n",
        "    decoder_inputs = [seq[:-1] for seq in pt_sequences]\n",
        "    decoder_outputs = [seq[1:] for seq in pt_sequences]\n",
        "\n",
        "    # Pad sequences\n",
        "    encoder_inputs = pad_sequences(fr_sequences, maxlen=max_input_length, padding='post')\n",
        "    decoder_inputs = pad_sequences(decoder_inputs, maxlen=(max_target_length - 1), padding='post')\n",
        "    decoder_outputs = pad_sequences(decoder_outputs, maxlen=(max_target_length - 1), padding='post')\n",
        "\n",
        "    return encoder_inputs, decoder_inputs, decoder_outputs, input_vocab_size, target_vocab_size\n"
      ],
      "metadata": {
        "id": "2dHvGPlSfaaX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 50\n",
        "max_target_length = 50"
      ],
      "metadata": {
        "id": "yBXIIu7XqYEh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoder_inputs, train_decoder_inputs, train_decoder_outputs, input_vocab_size, target_vocab_size = tokenize_and_pad(\n",
        "    fr_texts, pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "tqLUNrnaqOgZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_encoder_inputs, val_decoder_inputs, val_decoder_outputs, _, _ = tokenize_and_pad(\n",
        "    val_fr_texts, val_pt_texts, max_input_length, max_target_length\n",
        ")"
      ],
      "metadata": {
        "id": "esAVoW4ln0oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Modelo 1: LSTM BÃ¡sico\n",
        "\n",
        "No modelo LSTM, definimos a funÃ§Ã£o build_lstm_model que basicamente cria um encoder e um decoder usando teacher forcing. A ideia Ã© que o encoder pegue a sequÃªncia de entrada e o decoder aprenda a gerar a saÃ­da com base nas informaÃ§Ãµes que recebeu, sempre â€œcorrigindoâ€ o que jÃ¡ foi gerado. As camadas de inferÃªncia, que seriam usadas na hora de testar ou gerar traduÃ§Ãµes de verdade, a gente deixou de lado, mas o ChatGPT e o Deepseek recomendaram para gente, mas nÃ£o era o foco do trabalho."
      ],
      "metadata": {
        "id": "Efz62L4hU--R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(input_vocab_size, target_vocab_size,\n",
        "                     max_input_len, max_target_len,\n",
        "                     latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,), name=\"encoder_inputs\")\n",
        "    encoder_embed = Embedding(input_vocab_size, embedding_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_state=True, name=\"encoder_lstm\")\n",
        "    _, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len-1,), name=\"decoder_inputs\")\n",
        "    decoder_embed = Embedding(target_vocab_size, embedding_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True, name=\"decoder_lstm\")\n",
        "    decoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32', name=\"decoder_dense\")\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.encoder_inputs = encoder_inputs\n",
        "    model.encoder_embedding = model.get_layer(\"encoder_embedding\")\n",
        "    model.encoder_lstm = model.get_layer(\"encoder_lstm\")\n",
        "    model.decoder_embedding = model.get_layer(\"decoder_embedding\")\n",
        "    model.decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
        "    model.decoder_dense = decoder_dense\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "M6uiZBc2WLQF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando e Compilando os modelos\n",
        "\n",
        "Para todos os nossos modelos, a configuraÃ§Ã£o foi padrÃ£o: usamos o otimizador Adam com learning rate de 1e-4 e clipnorm de 1.0, a loss foi definida como 'sparse_categorical_crossentropy' e a mÃ©trica de acurÃ¡cia. Essa padronizaÃ§Ã£o ajudou a gente a comparar os resultados dos modelos sem ter que ficar ajustando vÃ¡rios parÃ¢metros diferentes."
      ],
      "metadata": {
        "id": "O-GvW_reWVTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = build_lstm_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_input_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4, clipnorm=1.0),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "6eN7iyFZWXkf",
        "outputId": "2d06b030-da6e-403c-ad0e-ddfcc6130541"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_inputs            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_inputs            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            â”‚              \u001b[38;5;34m0\u001b[0m â”‚ -                      â”‚\n",
              "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_embedding         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m151\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m9,464,576\u001b[0m â”‚ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_embedding         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m9,603,328\u001b[0m â”‚ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
              "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   â”‚        \u001b[38;5;34m525,312\u001b[0m â”‚ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
              "â”‚                           â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚        \u001b[38;5;34m525,312\u001b[0m â”‚ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ cast_2 (\u001b[38;5;33mCast\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚              \u001b[38;5;34m0\u001b[0m â”‚ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_dense (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m, \u001b[38;5;34m37513\u001b[0m)     â”‚      \u001b[38;5;34m9,640,841\u001b[0m â”‚ cast_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)              </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">        Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to           </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ encoder_inputs            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_inputs            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                      â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_embedding         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">151</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,464,576</span> â”‚ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_embedding         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,603,328</span> â”‚ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               â”‚                        â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
              "â”‚                           â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     â”‚                â”‚                        â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> â”‚ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    â”‚\n",
              "â”‚                           â”‚                        â”‚                â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ cast_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37513</span>)     â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,640,841</span> â”‚ cast_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,759,369\u001b[0m (113.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,759,369</span> (113.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando o Modelo"
      ],
      "metadata": {
        "id": "_4MEPbe8Wb9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento\n",
        "history = lstm_model.fit(\n",
        "    [train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\n",
        "    validation_data=([val_encoder_inputs, val_decoder_inputs], val_decoder_outputs),\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "vfhCfnnHWe7v",
        "outputId": "24485f9b-059f-4e28-954f-edaf0fd7cd0d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4388/4388\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8836 - loss: 1.8984"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-674e7d1cc367>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = lstm_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mtrain_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dec_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#[train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"functional_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 109) dtype=int32>]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Modelo 2: LSTM (Luong)\n",
        "\n",
        "A gente adaptou o modelo LSTM para incluir um mecanismo de atenÃ§Ã£o â€“ escolhemos o esquema de Luong mesmo, pois pareceu mais simples e direto. Aqui, em vez de usar teacher forcing, o decoder â€œpresta atenÃ§Ã£oâ€ nas partes relevantes do encoder durante a geraÃ§Ã£o da traduÃ§Ã£o. Foi interessante notar como a atenÃ§Ã£o ajudou a melhorar o alinhamento entre as palavras de entrada e saÃ­da, mesmo com uma estrutura parecida com a LSTM simples.\n"
      ],
      "metadata": {
        "id": "yNXz1Ebijgla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_attention_model(input_vocab_size, target_vocab_size, max_input_len, max_target_len, latent_units=256, embedding_dim=256):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,))\n",
        "    encoder_embed = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_units, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len - 1,))\n",
        "    decoder_embed = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(latent_units, return_sequences=True)\n",
        "    decoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Attention\n",
        "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
        "    decoder_concat = tf.concat([decoder_outputs, attention], axis=-1)\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax', dtype='float32')\n",
        "    decoder_outputs = decoder_dense(decoder_concat)\n",
        "\n",
        "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "52gXid9Uk7kq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model = build_lstm_attention_model(\n",
        "    input_vocab_size=len(fr_tokenizer.word_index)+1,\n",
        "    target_vocab_size=len(pt_tokenizer.word_index)+1,\n",
        "    max_input_len=max_fr,\n",
        "    max_target_len=max_pt\n",
        ")\n",
        "\n",
        "# optimizer=Adam(learning_rate=0.001)\n",
        "lstm_attn_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "lstm_attn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "dePkqj9kl2OZ",
        "outputId": "a5039c06-c3f7-4d71-ca5f-7eed5d0380f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'luong_attention' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Dimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'luong_attention', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  â€¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  â€¢ kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8865d15aeb9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m lstm_attn_model = build_lstm_luong_attention_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfr_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_source_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmax_target_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-c8788a8483d6>\u001b[0m in \u001b[0;36mbuild_lstm_luong_attention_model\u001b[0;34m(input_vocab_size, target_vocab_size, max_source_len, max_target_len, latent_units, embedding_dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLuongAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdec_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-ceb4c796fc11>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, query, values)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcontext_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LuongAttention.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'luong_attention' (of type LuongAttention). Either the `LuongAttention.call()` method is incorrect, or you need to implement the `LuongAttention.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 154 and 151 for '{{node add}} = AddV2[T=DT_HALF](ExpandDims, Placeholder_1)' with input shapes: [?,1,154,256], [?,151,256].\u001b[0m\n\nArguments received by LuongAttention.call():\n  â€¢ args=('<KerasTensor shape=(None, 154, 256), dtype=float16, sparse=False, name=keras_tensor_35>', '<KerasTensor shape=(None, 151, 256), dtype=float16, sparse=False, name=keras_tensor_30>')\n  â€¢ kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_attn_model.fit(\n",
        "    [train_encoder_inputs, train_decoder_inputs], train_decoder_outputs,\n",
        "    validation_data=([val_encoder_inputs, val_decoder_inputs], val_decoder_outputs),\n",
        "    epochs=5,\n",
        "    batch_size=10,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "FHfBYEr1aGzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Modelo 3: Transformador\n",
        "\n",
        "Por fim, implementamos um Transformer, que Ã© uma abordagem totalmente diferente, sem recorrÃªncia. No cÃ³digo, o encoder e o decoder sÃ£o feitos de vÃ¡rias camadas com MultiHeadAttention, normalizaÃ§Ã£o e dropout. Cada embedding Ã© escalado e normalizado antes de entrar nas camadas, e no final temos uma camada densa com softmax pra prever as palavras. Esse modelo chamou nossa atenÃ§Ã£o porque, apesar de mais complexo, mostrou um potencial Ãºnico pra capturar relaÃ§Ãµes mais profundas nas sentenÃ§as como o professor tinha dito em sala."
      ],
      "metadata": {
        "id": "I9Rs3XOzqqc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer_model(input_vocab_size, target_vocab_size, max_input_len, max_target_len, num_layers=2, d_model=256, num_heads=8, dff=512, dropout_rate=0.1):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,))\n",
        "    encoder_embed = Embedding(input_vocab_size, d_model)(encoder_inputs)\n",
        "    encoder_embed *= tf.math.sqrt(tf.cast(d_model, tf.float32))  # Scale embeddings\n",
        "    encoder_embed = LayerNormalization(epsilon=1e-6)(encoder_embed)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(encoder_embed, encoder_embed)\n",
        "        attn_output = Dropout(dropout_rate)(attn_output)\n",
        "        encoder_embed = LayerNormalization(epsilon=1e-6)(encoder_embed + attn_output)\n",
        "\n",
        "        ffn_output = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])(encoder_embed)\n",
        "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "        encoder_embed = LayerNormalization(epsilon=1e-6)(encoder_embed + ffn_output)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len - 1,))\n",
        "    decoder_embed = Embedding(target_vocab_size, d_model)(decoder_inputs)\n",
        "    decoder_embed *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    decoder_embed = LayerNormalization(epsilon=1e-6)(decoder_embed)\n",
        "\n",
        "    for _ in range(num_layers):\n",
        "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(decoder_embed, decoder_embed)\n",
        "        attn_output = Dropout(dropout_rate)(attn_output)\n",
        "        decoder_embed = LayerNormalization(epsilon=1e-6)(decoder_embed + attn_output)\n",
        "\n",
        "        ffn_output = tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])(decoder_embed)\n",
        "        ffn_output = Dropout(dropout_rate)(ffn_output)\n",
        "        decoder_embed = LayerNormalization(epsilon=1e-6)(decoder_embed + ffn_output)\n",
        "\n",
        "    # Final output\n",
        "    decoder_outputs = Dense(target_vocab_size, activation='softmax', dtype='float32')(decoder_embed)\n",
        "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "z8SbO511quGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = build_transformer_model(input_vocab_size, target_vocab_size, max_input_length, max_target_length)\n",
        "\n",
        "# optimizer=Adam(learning_rate=0.001)\n",
        "transformer_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "transformer_model.summary()"
      ],
      "metadata": {
        "id": "ldrnwqt9q2XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Š Visualizando os Resultados"
      ],
      "metadata": {
        "id": "im8JfjrfWjMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot da loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Treino')\n",
        "    plt.plot(history.history['val_loss'], label='ValidaÃ§Ã£o')\n",
        "    plt.title('Loss ao Longo do Treinamento')\n",
        "    plt.xlabel('Ã‰poca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot da acurÃ¡cia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Treino')\n",
        "    plt.plot(history.history['val_accuracy'], label='ValidaÃ§Ã£o')\n",
        "    plt.title('AcurÃ¡cia ao Longo do Treinamento')\n",
        "    plt.xlabel('Ã‰poca')\n",
        "    plt.ylabel('AcurÃ¡cia')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "70HJqHy1Wl86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_by_position(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Plots the token-level accuracy at each sequence position.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the ground truth token indices.\n",
        "      - y_pred: numpy array of shape (num_samples, sequence_length)\n",
        "                containing the predicted token indices.\n",
        "    \"\"\"\n",
        "    seq_length = y_true.shape[1]\n",
        "    accuracies = []\n",
        "    for pos in range(seq_length):\n",
        "        # Compute the fraction of tokens predicted correctly at this position.\n",
        "        pos_accuracy = np.mean(y_true[:, pos] == y_pred[:, pos])\n",
        "        accuracies.append(pos_accuracy)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(seq_length), accuracies, marker='o')\n",
        "    plt.xlabel(\"Token Position\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy by Token Position\")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Y-E3-rAeZ41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_length_distribution(y_true, y_pred):\n",
        "    # Calculando comprimentos (ignorando padding)\n",
        "    true_lengths = [len([x for x in seq if x != 0]) for seq in y_true]\n",
        "    pred_lengths = [len([x for x in seq if x != 0]) for seq in y_pred]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist([true_lengths, pred_lengths], label=['Real', 'Previsto'],\n",
        "             alpha=0.7, bins=20)\n",
        "    plt.title('DistribuiÃ§Ã£o do Comprimento das TraduÃ§Ãµes')\n",
        "    plt.xlabel('Comprimento da SequÃªncia')\n",
        "    plt.ylabel('FrequÃªncia')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8tlGUuaWb1i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_heatmap(y_true, y_pred, token_labels):\n",
        "    \"\"\"\n",
        "    Plots a confusion matrix (as a heatmap) for token predictions.\n",
        "\n",
        "    Parameters:\n",
        "      - y_true: numpy array of token indices (can be 2D or flattened)\n",
        "      - y_pred: numpy array of token indices (can be 2D or flattened)\n",
        "      - token_labels: list of strings that maps each token index to a label.\n",
        "                      For example: [\"PAD\", \"[START]\", \"[END]\", \"bonjour\", ...]\n",
        "    \"\"\"\n",
        "    # If the inputs are 2D (num_samples x seq_length), flatten them.\n",
        "    if y_true.ndim > 1:\n",
        "        y_true = y_true.flatten()\n",
        "    if y_pred.ndim > 1:\n",
        "        y_pred = y_pred.flatten()\n",
        "\n",
        "    # Compute the confusion matrix.\n",
        "    labels = np.arange(len(token_labels))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=token_labels,\n",
        "                yticklabels=token_labels)\n",
        "    plt.xlabel(\"Predicted Token\")\n",
        "    plt.ylabel(\"True Token\")\n",
        "    plt.title(\"Confusion Matrix of Token Predictions\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q7I9QRrpb32o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate predictions\n",
        "def generate_predictions(model, encoder_inputs, decoder_inputs, max_target_length):\n",
        "    predictions = model.predict([encoder_inputs, decoder_inputs])\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    return predictions\n",
        "\n",
        "# Token labels for confusion matrix (excluding padding token 0)\n",
        "token_labels = list(range(1, target_vocab_size))\n",
        "\n",
        "models = {\n",
        "    \"LSTM BÃ¡sico\": lstm_model,\n",
        "    \"LSTM (Luong)\": lstm_attn_model,\n",
        "    \"Transformer\": transformer_model\n",
        "}\n",
        "\n",
        "\n",
        "# Analyze each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nAnalyzing {name} model...\")\n",
        "\n",
        "    # Generate predictions\n",
        "    train_predictions = generate_predictions(model, train_encoder_inputs, train_decoder_inputs, max_target_length)\n",
        "    val_predictions = generate_predictions(model, val_encoder_inputs, val_decoder_inputs, max_target_length)\n",
        "\n",
        "    # Visualize training results\n",
        "    visualize_results(history)\n",
        "\n",
        "    # Plot accuracy by position\n",
        "    plot_accuracy_by_position(train_decoder_outputs, train_predictions)\n",
        "\n",
        "    # Plot sequence length distribution\n",
        "    plot_length_distribution(train_decoder_outputs, train_predictions)\n",
        "\n",
        "    # Plot confusion matrix heatmap\n",
        "    plot_confusion_heatmap(train_decoder_outputs, train_predictions, token_labels)"
      ],
      "metadata": {
        "id": "Vg1MAdWxZ-UV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}