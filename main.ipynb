{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Trabalho 1 - T√≥picos Especiais em Matem√°tica Aplicada\n",
        "\n",
        "**Alunos/Matricula:** Jo√£o V. Farias & Renan V. Guedes / 221022604 & 221031363\n",
        "\n",
        "**Arquitetura Usada:** Encoder-Decoder\n",
        "\n",
        "**Dataset Link:** V1: [D-Talk](https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatefr_to_pt) from TensorFlow.Datasets\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0iu8uQIc-P33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Projeto para traduzir mensagens do Franc√™s para o Portugues**  \n",
        "\n",
        "Neste projeto, vamos explorar e comparar tr√™s arquiteturas de redes neurais para tradu√ß√£o autom√°tica do franc√™s para o portugu√™s, usando o *dataset* TED Talks do *Open Translation Project*. A ideia √© testar modelos do tipo **Encoder-Decoder**, analisando suas diferen√ßas e impacto na qualidade da tradu√ß√£o.  \n",
        "\n",
        "Os tr√™s modelos que vamos treinar s√£o:  \n",
        "\n",
        "1. **LSTM (Long Short-Term Memory)**  \n",
        "   - Um modelo b√°sico de rede recorrente bidirecional. O **Encoder** processa a frase em franc√™s e gera um contexto, enquanto o **Decoder** usa esse contexto para formar a tradu√ß√£o em portugu√™s.  \n",
        "   - A principal vantagem desse modelo √© sua capacidade de lidar com depend√™ncias de longo prazo nas sequ√™ncias.  \n",
        "\n",
        "2. **LSTM com Mecanismos de Aten√ß√£o**  \n",
        "   - Uma vers√£o aprimorada do modelo anterior, adicionando camadas de aten√ß√£o (produto escalar, Bahdanau e Luong).  \n",
        "   - A aten√ß√£o ajuda o modelo a \"olhar\" para partes espec√≠ficas da frase de entrada enquanto traduz, melhorando a coer√™ncia e precis√£o.  \n",
        "\n",
        "3. **Transformers**  \n",
        "   - Uma abordagem mais moderna, baseada em **autoaten√ß√£o**, eliminando o uso de redes recorrentes.  \n",
        "   - Trabalha com processamento paralelo, usando *Multi-Head Attention* e *Positional Encoding* para entender rela√ß√µes entre palavras, mesmo quando est√£o distantes na frase.  \n",
        "\n",
        "**Como vamos testar os modelos?**  \n",
        "- **Dataset**: Vamos usar cerca de 52.000 pares de frases (franc√™s-portugu√™s) para treinar, al√©m de 1.200 para valida√ß√£o e 1.800 para teste.  \n",
        "- **Pr√©-processamento**: Faremos a tokeniza√ß√£o com *SubwordTextEncoder* para reduzir palavras fora do vocabul√°rio (*out-of-vocabulary* ‚Äì OOV).  \n",
        "- **Treinamento**: Otimiza√ß√£o com Adam, acompanhando a perda (*loss*) e a acur√°cia durante o processo.  \n",
        "- **Avalia√ß√£o**: Vamos comparar os resultados usando a m√©trica BLEU e analisar exemplos pr√°ticos das tradu√ß√µes.  \n",
        "\n",
        "**O que esperamos encontrar?**  \n",
        "- Nosso objetivo √© entender qual desses modelos tem o melhor equil√≠brio entre qualidade de tradu√ß√£o e efici√™ncia computacional.  \n",
        "- √â prov√°vel que os Transformers tenham um desempenho superior, j√° que conseguem processar frases de forma mais eficiente, enquanto os modelos com LSTM e aten√ß√£o devem mostrar um avan√ßo significativo sobre a vers√£o b√°sica de LSTM.  \n",
        "\n",
        "No fim das contas, essa an√°lise pode ajudar a compreender melhor como diferentes abordagens de deep learning se saem em tarefas de tradu√ß√£o, trazendo insights √∫teis para aplica√ß√µes reais em NLP.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Qqx0oHhH_YkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìö Importando as bibliotecas necess√°rias"
      ],
      "metadata": {
        "id": "rO-pcdsZGM-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1RAVeffMKsD",
        "outputId": "e6c0014d-7fa8-4cc4-8c9a-9793671c586f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.13.0\n"
          ]
        }
      ],
      "source": [
        "# Primeiro, vamos importar todas as bibliotecas que vamos precisar ao longo do Projeto\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Para processamento de texto\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Componentes do Keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, MultiHeadAttention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}